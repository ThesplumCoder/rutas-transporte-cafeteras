---
title: "Exploración de los datos de producción agrícola"
output:
  html_document: 
    toc: true
    number_sections: true
---

```{r echo=FALSE, results='hide', warning=FALSE, message=FALSE}
library(igraph)
library(ggplot2)
library(dplyr)
```

# Tratamiento de los datos

## Importación

Importamos los datos desde los archivos:
```{r}
eva_2007_2018 <- read.csv("datasets/evaluaciones-agropecuarias-municipales_2007-2018.csv")
coffee_production_antioquia <- read.csv("datasets/produccion-mensual-cafe-antioquia_1956-2019.csv")
coffee_production_antioquia <- coffee_production_antioquia[, -c(1)]
```

## Selección de datos relevantes

Separamos lo valores de la columna mes de los datos de producción mensual de 
café. Esto se hace para tener los años en una columna aparte:
```{r}
month_year <- strsplit(coffee_production_antioquia$Mes, split = "-")

parsedProduction <- c()
for(i in 1:length(coffee_production_antioquia$Producción)) {
  monthly_production <- unlist(strsplit(x = coffee_production_antioquia$Producción[i], split = ","))
  monthly_production <- paste(monthly_production, collapse = "")
  parsedProduction <- c(parsedProduction, monthly_production)
}
parsedProduction <- as.integer(parsedProduction)

coffee_production_antioquia <- cbind(coffee_production_antioquia, 
                                     data.frame(Producción = parsedProduction))
coffee_production_antioquia <- cbind(coffee_production_antioquia, 
                                     do.call(rbind, month_year))

coffee_production_antioquia <- coffee_production_antioquia[, -c(1, 2)]
colnames(coffee_production_antioquia) <- c("Producción(kilos)", "Mes", "Año")
coffee_production_antioquia$`Producción(kilos)` <- coffee_production_antioquia$`Producción(kilos)` * 60

colnames(eva_2007_2018)[13] = "PRODUCCIÓN(t)"
```

Como son muchas columnas, retiramos las columnas que no vamos a usar, nos 
quedamos con las que vamos a utilizar:
```{r}
eva_2007_2018 <- subset(eva_2007_2018, 
                        select = c("DEPARTAMENTO", 
                                   "MUNICIPIO", 
                                   "CULTIVO", 
                                   "AÑO", 
                                   "PRODUCCIÓN(t)"))
```

## Previsualización de los conjuntos de datos

Resúmen de cada una de las columnas  de los conjuntos de datos:
```{r}
summary(eva_2007_2018)
```

Vista previa a cada uno de los *dataframes*:
```{r}
head(eva_2007_2018)
head(coffee_production_antioquia)
```

# Exploración

Miramos cuáles son los cultivos que tienen más apariciones en los registros. 
Limitamos el gráfico a los 10 más presentes:
```{r}
crops_presence <- sort(table(eva_2007_2018$CULTIVO), decreasing = TRUE)

barplot(crops_presence[1:10], 
        ylim = c(0, 30000), 
        col = rainbow(10), 
        las = 3, 
        main = "Presencia de registros para cada cultivo en el país",
        xlab = "Cultivo",
        ylab = "Cantidad de registros")
```

Elegido el café como cultivo de interés, podemos dar un vistazo a los 
departamentos que tienen más municipios productores en promedio, a través de los 
años:
```{r}
coffee_producers <- subset(eva_2007_2018, CULTIVO == "CAFE")
years_range <- 11
departments_presence <- sort(table(coffee_producers$DEPARTAMENTO) / years_range, decreasing = TRUE)

barplot(departments_presence, 
        ylim = c(0, 100), 
        col = rainbow(32), 
        las = 3, 
        xpd = TRUE, 
        main = "Municipios productores",
        xlab = "Departamento",
        ylab = "Media de municipios")
```

Como Antioquia es el departamento con más presente como productor de café a 
través de los años, observamos su tendencia de producción. Esto con el fin de 
poder modelar la función de producción, que nos servirá para la simulación.
```{r}
antioquia_coffee <- subset(coffee_producers, DEPARTAMENTO == "ANTIOQUIA")

production_2018 <- subset(antioquia_coffee, AÑO == 2018)
production_2017 <- subset(antioquia_coffee, AÑO == 2017)
plot(production_2018$`PRODUCCIÓN(t)`)
lines(production_2017$`PRODUCCIÓN(t)`, col = "blue")
```

Utilizamos el conjunto de datos que tiene observaciones de la producción mensual 
de café verde, en kilogramos.
```{r}
hist(coffee_production_antioquia$`Producción(kilos)`,
     breaks = 20,
     col = "sandybrown",
     main = "Producción mensual de café verde",
     xlab = "Kilogramos",
     ylab = "Frecuencia absoluta")
```

# Generadores y ajustes

## Generación de producción de café

En la exploración de los datos de producción de café para Antioquia, vemos un 
patrón reconocible. Los datos parece que siguen una distribución de probabilidad 
distinguible.

Para construir la función de generación aleatoria de producción de café vamos a 
utilizar la función de probabilidad de Weibull. La función de distribución 
acumulada de Weibull que vamos a usar es:
$$
F_{(x)} = 1 - e^{-(\lambda x)^\alpha}
$$
Implementamos la función de densidad y la función de distribución de Weibull.
```{r}
density_weibull <- function(x, lambda, alpha) {
  lambda * alpha * ((lambda * x) ** (alpha - 1)) * exp(-(lambda * x) ** alpha)
}

cummulative_weibull <- function(x, lambda, alpha) {
  1 - exp(-(lambda * x) ** alpha)
}
```

### Uso de la función inversa para la función generadora

Función generadora utilizando la función inversa.
$$
F_{(x)} = 1 - e^{-(\lambda x)^\alpha}, x > 0 \\
U = 1 - e^{-(\lambda x)^\alpha} \\
e^{-(\lambda x)^\alpha} = 1 - U \\
-(\lambda x)^\alpha = Ln(1 - U) \\
\lambda x = (-Ln(1 - U))^{1 / \alpha} \\
x = \frac{{(-Ln(1 - U))}^{1/ \alpha}}{\lambda}
$$

```{r}
random_weibull <- function(lambda, alpha) {
  x <- ((-log(1 - runif(1))) ** (1 / alpha)) / (lambda)
  
  return(x)
}
```

Se prueba para una forma conocida de la función de densidad, en este caso con 
los parámetros de $\lambda = 1$ y $\alpha = 1.5$.
```{r}
tests <- c()
for (iter in 1:1000) {
  tests <- c(tests, random_weibull(lambda = 1, alpha = 1.5))
}

hist(tests,
     freq = FALSE,
     xlim = c(0, 2.5),
     ylim = c(0, 1),
     breaks = 15,
     col = "lightskyblue",
     main = "Función de generación aletoria de Weibull",
     xlab = "x", 
     ylab = "Densidad de probabilidad")
curve(density_weibull(x, lambda = 1, alpha = 1.5), 
      from = 0, 
      to = 2.5,
      col = "red",
      add = TRUE)
```


### Ajuste usando el estimador de máxima verosimilitud

Se construye la función que optimiza el parámetro $\lambda$ para la función de 
distribución de Weibull.
```{r}
optimize_lambda <- function(coffee_production, alpha) {
  lambda = ((length(coffee_production) * alpha) / (alpha * sum(coffee_production ** alpha))) ** (1 / alpha)
  
  return(lambda)
}
```

Con el ajuste creado, construimos la función que nos va a generar la producción 
de café. Esta función se basa en el parámetro arbitrario que definamos para 
$\alpha$ y optimiza $\lambda$.
```{r}
production_generator <- function(coffee_data, alpha_weibull, amount) {
  lambda <- optimize_lambda(coffee_production = coffee_data, alpha = alpha_weibull)
  
  coffee_production <- c()
  for (iter in 1:amount) {
    coffee_production <- c(coffee_production, random_weibull(lambda = lambda, alpha = alpha_weibull))
  }
  
  return(coffee_production)
}
```

Se prueba la optimización y la función de generación aleatoria de producción.
```{r}
coffee_tests <- production_generator(coffee_data = coffee_production_antioquia$`Producción(kilos)`, 
                                     alpha_weibull = 2.2,
                                     amount = 10000)
hist(coffee_tests,
     breaks = 20,
     col = "lightskyblue",
     main = "Función de generación aletoria de producción de café",
     xlab = "Kilos de café", 
     ylab = "Frecuencia absoluta")
```

Comparamos con los datos reales:
```{r}
par(mfrow = c(1, 2))
boxplot(coffee_tests,
        col = "lightskyblue",
        main = "Producción simulada")
boxplot(coffee_production_antioquia$`Producción(kilos)`,
        col = "sandybrown",
        main = "Producción observada")
```

### Generalización para otros departamentos

Utilizamos la prueba estadística de bondad de ajuste para establecer un criterio 
de aceptación de los parámetros que elegimos para el ajuste.

Implementamos la función para calcular el estadístico $\chi^2$.
```{r}
fit_test <- function(observed_data, expected_data) {
  partitions <- length(observed_data)
  significance <- 0.01
  
  chisq_test <- sum(((observed_data - expected_data) ** 2) / expected_data)
  p_value <- qchisq(p = significance, 
                    df = partitions - 1, 
                    lower.tail = FALSE)
  
  results <- c(chisq_test, p_value)
  return(results)
}
```

Implementamos una función que nos permita encontrar las frecuencias de datos en 
cada una de las particiones que definamos. Nos devolverá una matriz, en la 
primera columna vendrán las cantidades esperadas; en consecuencia, en la segunda 
vendrán las cantidades obtenidas.
```{r}
partition_calculator <- function(observed_data, lambda_weibull, alpha_weibull, partitions) {
  expected_values <- c()
  obtained_values <- c()
  
  size <- max(observed_data, na.rm = TRUE) / partitions
  lower_limit <- 0
  upper_limit <- size
  for (partition in 1:partitions) {
    lower_percent <- cummulative_weibull(lower_limit, lambda = lambda_weibull, alpha = alpha_weibull)
    upper_percent <- cummulative_weibull(upper_limit, lambda = lambda_weibull, alpha = alpha_weibull)
    percent <- upper_percent - lower_percent
    
    obtained_values <- c(obtained_values, 
                         sum(observed_data >= lower_limit & observed_data < upper_limit))
    expected_values <- c(expected_values, 
                         percent * length(observed_data))
    
    lower_limit <- upper_limit
    upper_limit <- upper_limit + size
  }
  
  return(cbind(expected_values, obtained_values))
}
```

Calculamos las cantidades esperadas (función de Weibull ajustada) y las 
cantidades observadas (datos reales).
```{r}
partition_values <-partition_calculator(observed_data = coffee_production_antioquia$`Producción(kilos)`, 
                                        lambda_weibull = optimize_lambda(coffee_production =  coffee_production_antioquia$`Producción(kilos)`, alpha = 2.2),
                                        alpha_weibull = 2.2, 
                                        partitions = 10)


matplot(partition_values, 
        type = "l", 
        col = c("lightskyblue", "sandybrown"), 
        main = "Comparación entre valores observados vs. esperados", 
        xlab = "Partición", 
        ylab = "Frecuencia absoluta")
legend("topright", 
       legend = c("Observados", "Esperados"), 
       lty = 1, 
       col = c("sandybrown", "lightskyblue"))

chi_test <- fit_test(observed_data = partition_values[, 2], expected_data = partition_values[, 1])
curve(dchisq(x, df = 9), 
      from = 0, 
      to = chi_test[1] + 1,
      main = "Prueba de bondad de ajuste para los datos de Antioquia",
      xlab = "x",
      ylab = "Densidad de probabilidad")
abline(v = chi_test[1], col = "green")
abline(v = chi_test[2], col = "red")
```

## Cálculo de rutas eficientes
Para el valor de los peajes se tomó la información presente en el dataset [Tarifas de Peajes ANI](https://www.datos.gov.co/Transporte/Tarifas-de-Peajes-ANI/7gj8-j6i3/about_data), que correspondiera a la tarifa de categoría 3 (Camiones de 3 y 4 ejes).
```{r}
tolls <- read.csv("datasets/Tarifas_de_Peajes_ANI_20240708.csv")
```

```{r}
hist(tolls$Valor, probability = TRUE, main = "Valores de peajes (Categoria III)")
curve(dnorm(x, mean = mean(tolls$Valor), sd = sd(tolls$Valor)), from = min(tolls$Valor), to = max(tolls$Valor), add = TRUE, col = "red")
```

Como los datos se acomodan moderadamente a la distribución normal, se asignarán los valores de los peajes gracias a runif() y teniendo en cuenta el número de presentes en cada tramo.

```{r}
cant_peajes <- c(5, 4, 3, 13, 2, 2, 18, 3, 9, 12, 12, 6, 9, 7, 17, 11, 5, 9, 7, 21, 9, 14, 14, 17, 6, 17, 18, 6, 6, 5, 9, 12, 9, 11, 7, 7, 2, 3, 2, 14, 3, 5, 15, 8, 5, 7, 5, 9, 5, 10, 12, 3, 2, 10, 10, 16, 16, 19, 4, 18, 19, 2, 10, 19, 1)

sum_rnorm <- function(n) {
  sum(rnorm(n, mean = mean(tolls$Valor), sd = sd(tolls$Valor)))
}
```

```{r}
cities <- c("Medellin", "Bucaramanga", "Bogota", "Tunja", "Pasto", "Puerto_Cartagena", "Puerto_Barranquilla", "Cali", "Ibague", "Cucuta", "Neiva", "Popayan")

rutas <- data.frame(
  origen = c(cities[1], cities[1], cities[1], cities[1], cities[2], cities[2], cities[2], cities[3], cities[3], cities[4], cities[6], cities[6], cities[6], cities[6], cities[6], cities[7], cities[7], cities[7], cities[7], cities[7], cities[8], cities[8], cities[8], cities[8], cities[8], cities[8], cities[8], cities[9], cities[9], cities[9], cities[9], cities[9], cities[9], cities[9], cities[9], cities[10], cities[10], cities[10], cities[10], cities[10], cities[10], cities[10], cities[10], cities[10], cities[11], cities[11], cities[11], cities[11], cities[11], cities[11], cities[11], cities[11], cities[11], cities[11], cities[12], cities[12], cities[12], cities[12], cities[12], cities[12], cities[12], cities[12], cities[12], cities[12], cities[12]),
  destino = c(cities[2], cities[3], cities[4], cities[5], cities[3], cities[4], cities[5], cities[4], cities[5], cities[5], cities[1], cities[2], cities[3], cities[4], cities[5], cities[1], cities[2], cities[3], cities[4], cities[5], cities[1], cities[2], cities[3], cities[4], cities[5], cities[6], cities[7], cities[1], cities[2], cities[3], cities[4], cities[5], cities[6], cities[7], cities[8], cities[1], cities[2], cities[3], cities[4], cities[5], cities[6], cities[7], cities[8], cities[9], cities[1], cities[2], cities[3], cities[4], cities[5], cities[6], cities[7], cities[8], cities[9], cities[10], cities[1], cities[2], cities[3], cities[4], cities[5], cities[6], cities[7], cities[8], cities[9], cities[10], cities[11]),
  weight = c(383, 432, 413, 813, 397, 282, 1150, 141, 828, 961, 663, 633, 1065, 908, 1523, 700, 577, 1009, 852, 1633, 443, 763, 457, 588, 384, 1102, 1139, 365, 509, 211, 341, 636, 1050, 1000, 261, 578, 197, 567, 428, 1383, 723, 668, 968, 711, 585, 676, 322, 452, 516, 1217, 1161, 387, 210, 874, 571, 891, 588, 717, 244, 1231, 1267, 141, 383, 1089, 271)
)
```

Tomando como base que en promedio para cada 100km de recorrido, estos camiones consumen 9.25 galones de Diesel y el precio por galón es en promedio de 9065 pesos colombianos, podemos encontrar el valor de cada tramo en función de su costo por combustible.
```{r}
cant_combustible <- rutas$weight*0.09246022 #Conversión a galones
costo_combustible <- cant_combustible*9065 #Conversión a precio en pesos Colombianos
```

```{r}
# Crear el grafo
G <- graph_from_data_frame(rutas, directed = FALSE, vertices = data.frame(cities))
plot(G)
```

Finalmente, se suman los valores de combustible y peajes obtenidos para cada tramo. Y se obtiene el valor del mejor recorrido para un tramo.

```{r}
best_route <- function(from, to) {
  valor_peajes <- sapply(cant_peajes, sum_rnorm)
  rutas$weight <- costo_combustible + valor_peajes
  G <- graph_from_data_frame(rutas, directed = FALSE, vertices = data.frame(cities))
  
  sp <- shortest_paths(G, from = from, to = to, weights = E(G)$weight, algorithm = "dijkstra", output = "both")
  
  path_vertices <- sp$vpath[[1]]
  path_cities <- V(G)$name[path_vertices]
  
  total <- sum(E(G, path = path_vertices)$weight)
  
  list(path_cities = path_cities, total = total)
}
```

La función retorna las ciudades por las que se pasa y la cantidad de kilómetros por recorrer.

```{r}
#ex
best_route("Medellin", "Puerto_Cartagena")
```

```{r}
efficient_route <- function() {
  return(500000)
}
```

## Costos de transporte

Con el cálculo de las rutas eficientes según ciertas condiciones, se responderán 
las siguientes preguntas: 

+ ¿Cuál es el costo promedio por departamento? Esto nos sirve para definir el 
departamento que es el productor con la distribución más eficiente.

Definimos la función que repartirá la carga entre los camiones que necesitará el 
departamento.
```{r}
load_distributor <- function(coffee_production) {
  maximun_load <- 18000
  trucks <- ceiling(coffee_production / maximun_load)
  
  return(trucks)
}
```

Definimos la función de costo que utilizaremos:
$$
costo = camiones \cdot costo\ ruta\ eficiente
$$

```{r}
cost <- function(trucks, cost_efficient_route) {
  cost <- trucks * cost_efficient_route
  
  return(cost)
}
```

# Simulación

## Antioquia

Para el caso de Antioquia vamos a simular $10000$ producciones mensuales de café 
y vamos a observar sus resultados.
```{r}
repetitions <- 100000
monthly_costs <- c()
monthly_production <- production_generator(coffee_data = coffee_production_antioquia$`Producción(kilos)`, 
                                           alpha_weibull = 2.2, 
                                           amount = repetitions)

for (iter in 1:repetitions) {
  trucks <- load_distributor(coffee_production = monthly_production[iter])
  monthly_costs <- c(monthly_costs, 
                     cost(trucks = trucks, cost_efficient_route = efficient_route()))
}

hist(monthly_costs / 1000000)
```

## Santander

```{r}
coffee_production_santander <- subset(coffee_producers)
```

